---
layout: default
title: "The Gaze Dialogue Model: Nonverbal Communication in HHI and HRI"
---

<center><h1>{{ page.title }}</h1></center>

<td>
	<center>
		IEEE Transactions on Cybernetics 2022 <br>
		<br>
		<nobr>Nuno Ferreira Duarte</nobr> &emsp;&emsp; <nobr>Mirko Raković (1)</nobr> &emsp;&emsp; <nobr>Jorge Marques </nobr> &emsp;&emsp; <nobr> Aude Billard (2) </nobr> s&emsp;&emsp; <nobr> José Santos-Victor </nobr> &emsp;&emsp; <nobr>(2) EPFL </nobr> <br>
		<nobr> University of Lisboa </nobr> &emsp;&emsp; <nobr>(1) University of Novi Sad</nobr> <br>
		<br>
		<img style="vertical-align:middle" src="eg3.png"  width="100%" height="inherit"/>		
	</center>
</td>

<br>
	
<td>
	<hr>
	<h3 style="margin-bottom:10px;">Abstract</h3>
		When humans interact with each other, eye gaze movements have to support motor control as well as communication. On the one hand, we need to fixate the task goal to retrieve visual information required for safe and precise action-execution. On the other hand, gaze movements fulfil the purpose of communication, both for reading the intention of our interaction partners, as well as to signal our action intentions to others. We study this Gaze Dialogue between two participants working on a collaborative task involving two types of actions: 1) individual action and 2) action-in-interaction . We recorded the eye-gaze data of both participants during the interaction sessions in order to build a computational model, the Gaze Dialogue , encoding the interplay of the eye movements during the dyadic interaction. The model also captures the correlation between the different gaze fixation points and the nature of the action. This knowledge is used to infer the type of action performed by an individual. We validated the model against the recorded eye-gaze behavior of one subject, taking the eye-gaze behavior of the other subject as the input. Finally, we used the model to design a humanoid robot controller that provides interpersonal gaze coordination in human–robot interaction scenarios. During the interaction, the robot is able to: 1) adequately infer the human action from gaze cues; 2) adjust its gaze fixation according to the human eye-gaze behavior; and 3) signal nonverbal cues that correlate with the robot’s own action intentions.
</td>

<td>
	<h3> Paper: [<a href="GazeDialogue_2022.pdf">PDF</a>] &nbsp; &nbsp; &nbsp; </h3><!-- Code: [<a href="https://github.com/NunoDuarte">GitHub</a>]</h3> -->
</td>

<tr>
		<h3 style="margin-bottom:10px;">Videos</h3>
<iframe src="https://www.youtube.com/embed/awneY9s6Zqk" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>	
</tr>
	
<br>
<br>

<h3 style="margin-bottom:0px;">Bibtex</h3>
<pre>
@ARTICLE{9965577,
  author={Raković, Mirko and Duarte, Nuno Ferreira and Marques, Jorge and Billard, Aude and Santos-Victor, José},
  journal={IEEE Transactions on Cybernetics}, 
  title={The Gaze Dialogue Model: Nonverbal Communication in HHI and HRI}, 
  year={2022},
  volume={},
  number={},
  pages={1-0},
  doi={10.1109/TCYB.2022.3222077}}
</pre>
