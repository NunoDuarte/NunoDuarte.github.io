---
layout: default
title: "Action Anticipation: Reading the Intentions of Humans and Robots"
---

<center><h1>{{ page.title }}</h1></center>

<td>
	<center>
		 IEEE Robotics and Automation Letters ( Volume: 3 , Issue: 4 , Oct. 2018 ) <br>
		<em>  <br>
		<br>
		<nobr>Nuno Ferreira Duarte</nobr> &emsp;&emsp; <nobr>Mirko Raković (1)  &emsp;&emsp; Jovica Tasevski (1) &emsp;&emsp;  Moreno Ignazio Coco (2) &emsp;&emsp;  Aude Billard (3) J&emsp;&emsp; José Santos-Victor </nobr> <br>
		<nobr> Universidade de Lisboa &emsp;&emsp; (1) University of Novi Sad  &emsp;&emsp; (2) University of Edinburgh &emsp;&emsp; (3) École Polytechnique Fédérale de Lausanne </nobr><br>
		<br>
		<img style="vertical-align:middle" src="actionspace_teaser.png"  width="100%" height="inherit" />		
	</center>
</td>

<br>
	
<td>
	<hr>
	<h3 style="margin-bottom:10px;">Abstract</h3>
	Humans have the fascinating capacity of processing nonverbal visual cues to understand and anticipate the actions of other humans. This “intention reading” ability is underpinned by shared motor repertoires and action models, which we use to interpret the intentions of others as if they were our own. We investigate how different cues contribute to the legibility of human actions during interpersonal interactions. Our first contribution is a publicly available dataset with recordings of human body motion and eye gaze, acquired in an experimental scenario with an actor interacting with three subjects. From these data, we conducted a human study to analyze the importance of different nonverbal cues for action perception. As our second contribution, we used motion/gaze recordings to build a computational model describing the interaction between two persons. As a third contribution, we embedded this model in the controller of an iCub humanoid robot and conducted a second human study, in the same scenario with the robot as an actor, to validate the model's “intention reading” capability. Our results show that it is possible to model (nonverbal) signals exchanged by humans during interaction, and how to incorporate such a mechanism in robotic systems with the twin goal of being able to “read” human action intentionsand acting in a way that is legible by humans.

</td>

<td>
	<h3> Paper: [<a href="RAL_2018.pdf">PDF</a>] </h3>
</td>

<tr>
	<h3 style="margin-bottom:10px;">Video</h3>
<iframe width="560" height="315" src="https://www.youtube.com/embed/HirRPgZGgFA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</tr>
	
<br>
<br>

<h3 style="margin-bottom:0px;">Bibtex</h3>
<pre>
@ARTICLE{8423498, 
author={N. F. {Duarte} and M. {Raković} and J. {Tasevski} and M. I. {Coco} and A. {Billard} and J. {Santos-Victor}}, 
journal={IEEE Robotics and Automation Letters}, 
title={Action Anticipation: Reading the Intentions of Humans and Robots}, 
year={2018}, 
volume={3}, 
number={4}, 
pages={4132-4139}, 
keywords={control engineering;gaze tracking;human computer interaction;humanoid robots;robot vision;sensor fusion;visual perception;action anticipation;nonverbal visual cues;intention reading ability;shared motor repertoires;action models;human body motion;eye gaze;action perception;robot intentions;human intentions;nonverbal cues;motion recordings;gaze recordings;iCub humanoid robot controller;sensor fusion;Robot kinematics;Computational modeling;Robot sensing systems;Solid modeling;Tracking;Magnetic heads;Social human-robot interaction;humanoid robots;sensor fusion}, 
doi={10.1109/LRA.2018.2861569}, 
ISSN={2377-3766}, 
month={Oct},}
</pre>
